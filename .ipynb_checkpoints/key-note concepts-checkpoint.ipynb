{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle\n",
    "# outlier in CV set or test set(ML and time series)\n",
    "# data drift\n",
    "# HPs, generalization and convergence\n",
    "# optimum HPs(can be done using only training data) and optimum HPs for generalization(should be done using multiple unseen samples like K-fold CV)\n",
    "# central limit theorem and before theorems\n",
    "# why iid ?\n",
    "# if there is unseen data(new categories/range of continuous values) in test, distribution(multivariate) of input seem to be changed, how significantly it has changed decides the model performance\n",
    "# is correlation enough to predict(univariate)or regression is needed to form an eqn, relationship between correlation and regression\n",
    "# interpretable AI\n",
    "# model training through data snapshots\n",
    "# range of model inputs and outputs(intrapolation and extrapolation in all models)\n",
    "# data having too much variance/different patterns(how do we find this out) should be clusteted/segmented together and should be modelled\n",
    "# clusters are normally created when data is huge rowwise and multidimensional(multiple features  create variance in the data , ex: weather variables differ for different regions)\n",
    "# Recognising the difference a pattern and a bias in the data\n",
    "# dont create features from target variable(creating weather features needs target duration or gap)even there are multiple weather features, complex algorithms can learn that they are created/sum up to target duration and hence inflate the models and create misinterpretation and misleading results\n",
    "# creating a feature and its derived feature from future is data leakage(snapshot tenure and total tenure)\n",
    "\n",
    "##the way data is collected and how problem is formulated might also the reason for lack of patterns in the data\n",
    "##agg model is created without considering how many projects were happening in how many sites while doing the aggregation/creating aggregate dataset, so this was as good as saying previous value as the  better forecast, peak was due to multiple projects happening at the same time in the middle phase and in the end the peak reduced as projects were ending\n",
    "\n",
    "\n",
    "##bias is used in two different contexts\n",
    "#1)estimation - difference from popn parameter, difference from target fun\n",
    "#2)inclination - sampling bias, model selection bias(choosing random forest without even checking the data)\n",
    "\n",
    "##variance:\n",
    "#std_error reflects the variance in the data, when the different fold(fold containing outlier, different distribution of features) is introduced to the model what is the metric(statistic in stats) how good the model behaves, this gives a picture how different model moght behave on a (variation)a unseen data.\n",
    "#std_error in ML is std_error for metrices(like MAE) in different folds(corresponding idea is std_error of statistc in a sampling distribution) \n",
    "##if metric like R2 does not follow any sampling distribution, we can go for non-parametric tests to find CI of metrics\n",
    "\n",
    "\n",
    "##descriptive stats give facts and the truth, so we need not assume/infer anything about bigger picture of data/popn distribution\n",
    "##inference stats is mostly used when we dont have enough evidence/confidence about data in hand and want to infer about the bigger picture/popln using some confidence(CI)which consideres variation(std_error) in different data samples(sampling techniques)\n",
    "\n",
    "#when there is enough data and hence evidence about some effect or pattern, we need not conduct a statistical test such as hypothesis tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training on a dynamically changing feature(no of sites pending feature)\n",
    "# training weekly data extracts(check for dynamic data, sampling bias as EDA/sanity checks)-soln-training using snapshot concatenation like HR snapshot training approach\n",
    "# historic data for agg model:using old similar projects(modernization,expansion etc)\n",
    "# assert\n",
    "# is data behaving same as business understanding(intital analysis engine)ex:plo sequence\n",
    "\n",
    "# issues on prod:\n",
    "# 1.package version change\n",
    "# 2.late updates and model performance\n",
    "# 3.difference in count in databricks and local(felt there was a decrese in counts than last weeks)\n",
    "# 4.difference in count of agg and site model due to linearpolation for missing value imputation in agg model, for model we should use imputation, for validation we should use actual even if it zero\n",
    "\n",
    "# interpretation of results(very important)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
